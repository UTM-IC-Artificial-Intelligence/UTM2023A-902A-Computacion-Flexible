{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RN Backprogation online - step to step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato de entrenamiento del ejemplo:\n",
    "\n",
    "| $x_1$ | $x_2$ | $t_1$ | $t_2$|\n",
    "|---|---|---|---|\n",
    "| 0.05 | 0.1 | 0.01 | 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = np.array([0.05, 0.1])\n",
    "target = np.array([0.01, 0.99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura de la Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo la arquitectura de la RN es la siguiente:\n",
    "\n",
    "![RN s2s online](../img/rn_online_s2s.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La imagen anterior tiene una capa de entrada, una capa oculta y una capa de salida. Las unidades en la capa oculta están completamente conectada a la capa de entrada y a la capa de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar la arquitectura de la red se utiliza una matriz que represente el conjunto de pesos entre dos capas. Por ejemplo, para los pesos entre la capa de entrada $i$ y la capa oculta $h$, se utiliza la matriz:\n",
    "\n",
    "$ W^{(h)} = \\begin{bmatrix} 0.15 & 0.25 & 0.35 \\\\ 0.20 & 0.30 & 0.35 \\end{bmatrix}$\n",
    "\n",
    "Nótese que se implementa una columna adicional para los pesos de la neurona bias de la capa de entrada que conecta con la capa oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_h = np.array([[0.15, 0.25, 0.35],\n",
    "                [0.20, 0.30, 0.35]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para representar los pesos entre la capa oculta y la capa de salida, de forma similar, se utiliza una matriz que representa los conexiones entre ambas capas:\n",
    "\n",
    "$ W^{(out)} = \\begin{bmatrix} 0.40 & 0.50 & 0.60 \\\\ 0.45 & 0.55 & 0.60 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_out = np.array([[0.40, 0.50, 0.60], \n",
    "                 [0.45, 0.55, 0.60]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para conocer la salida de la RN se propagan los patrones de la muestra de entrenamiento hacia adelante de la red comenzando desde la capa de entrada.\n",
    "\n",
    "### Activación de la capa de entrada\n",
    "\n",
    "Para calcular la activación de las neuronas de la capa de entrada $a_i^{(in)}$ se consideran como tal las mismas entradas, es decir, el mismo patrón que se presenta a la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrada de las neuronas de la capa de entrada\n",
    "z_in_i1 = pattern[0]\n",
    "z_in_i2 = pattern[1]\n",
    "\n",
    "# Salida de las neronas de la capa de entrada\n",
    "a_in_i1 = z_in_i1\n",
    "a_in_i2 = z_in_i2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activación de la capa oculta\n",
    "\n",
    "La activación de las neuronas de la capa oculta $a_i{(h)}$ se realiza mediante la suma de la propagación de la capa anterior (capa de entrada) hacia la neurona que se desea calcular la entrada. En otras palabras, para la activación de la neurona uno $a_1^{(h)}$  el valor de entrada $z_1^{(h)}$ mediante la fórmula: \n",
    "$z_1^{(h)} = a_0^{(in)}w_{0,1}^{(h)}+a_1^{(in)}w_{1,1}^{(h)}+ \\cdots +a_m^{(in)}w_{m,1}^{(h)} = \\sum_{i=0}^m a_i^{(in)}w_{i,1}$.\n",
    "\n",
    "Para calcular la salida de las neurona de la capa oculta, se utiliza la función sigmoidea: $\\phi\\big(z\\big) = \\frac{1}{1 + e^{-z}}$, entoces la salida de las neurona uno se calcula mediante:  $a_1^{(h)} = \\phi\\big(z_1^{(h)}\\big)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada h1 =  0.3825\n",
      "Entrada h2 =  0.39\n",
      "Salida h1 =  0.59448\n",
      "Salida h2 =  0.59628\n"
     ]
    }
   ],
   "source": [
    "# Entrada de las neuronas de la capa oculta\n",
    "z_h_h1 = round(a_in_i1 * w_h[0][0] + a_in_i2 * w_h[0][1] + 1 * w_h[0][2], 5)\n",
    "z_h_h2 = round(a_in_i1 * w_h[1][0] + a_in_i2 * w_h[1][1] + 1 * w_h[1][2], 5)\n",
    "\n",
    "print(\"Entrada h1 = \", z_h_h1)\n",
    "print(\"Entrada h2 = \", z_h_h2)\n",
    "\n",
    "# Salida de las neuronas de la capa oculta\n",
    "a_h_h1 = round(1 / (1 + math.exp(- z_h_h1)), 5)\n",
    "a_h_h2 = round(1 / (1 + math.exp(- z_h_h2)), 5)\n",
    "\n",
    "print(\"Salida h1 = \", a_h_h1)\n",
    "print(\"Salida h2 = \", a_h_h2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activación de la capa de salida\n",
    "\n",
    "Para la activación de las neuronas de la capa de salida $a_i^{(out)}$, el cálculo de las entradas y salidas de cada una de las neuronas se realiza de la misma forma que en la capa oculta. Ejemplo, para calcular la activación $a_1^{(out)}$ de la neurona 1, se considera el valor de entrada $z_1^{(out)}$:\n",
    "\n",
    "$z_1^{(out)} = a_0^{(h)}w_{0,1}^{(out)}+a_1^{(h)}w_{1,1}^{(out)}+ \\cdots +a_m^{(h)}w_{m,1}^{(out)}$\n",
    "\n",
    "Para calcular la salida de las neurona de la capa de salida, nuevamente se utiliza la función sigmoidea: $\\phi\\big(z\\big) = \\frac{1}{1 + e^{-z}}$, entoces la salida de las neurona uno se calcula mediante:  $a_1^{(out)} = \\phi\\big(z_1^{(h)}\\big)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada o1 =  1.13593\n",
      "Entrada o2 =  1.19547\n",
      "Salida o1 =  0.75693\n",
      "Salida o2 =  0.76772\n"
     ]
    }
   ],
   "source": [
    "# Entrada a las neuronas de la capa oculta\n",
    "z_out_o1 = round(a_h_h1 * w_out[0][0] + a_h_h2 * w_out[0][1] + 1 * w_out[0][2], 5)\n",
    "z_out_o2 = round(a_h_h1 * w_out[1][0] + a_h_h2 * w_out[1][1] + 1 * w_out[1][2], 5)\n",
    "\n",
    "print(\"Entrada o1 = \", z_out_o1)\n",
    "print(\"Entrada o2 = \", z_out_o2)\n",
    "\n",
    "# Salida de las neuronas de la capa oculta\n",
    "a_out_o1 = round(1 / (1 + math.exp(- z_out_o1)), 5)\n",
    "a_out_o2 = round(1 / (1 + math.exp(- z_out_o2)), 5)\n",
    "\n",
    "print(\"Salida o1 = \", a_out_o1)\n",
    "print(\"Salida o2 = \", a_out_o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cálculo del error de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error específico\n",
    "\n",
    "Una vez obtenidos los valores de activación de la capa de salida, se considera que estos son los valores de salida actuales de la red. Por lo que es necesario cálcular el error de la red, considerando los valores de salida esperados.\n",
    "\n",
    "Primero se calcula el error específico del patrón actual que fue presentada a la red durante el algoritmo de feedforward. El error de una neurona de la capa de salida, se calcula mediante: $error_{a_i^{(out)}} = target_i - a_i^{(out)}$. Sin embargo, es necesario conocer el error previo a la etapa de activación y dado que la activación se realiza con la función sigmoidea: $\\phi\\big(z\\big) = \\frac{1}{1 + e^{-z}}$ entonces se utiliza su derivada ($\\phi'\\big(z\\big) = \\phi\\big(z\\big) \\cdot (1- \\phi\\big(z\\big))$), con el fin de conocer el error previo a la aplicación de la función sigmoide. La forma de calcular el error es la siguiente:\n",
    "\n",
    "$error_{a_i^{(out)}} = (target_i -  a_i^{(out)}) *  a_i^{(out)} * (1 -  a_i^{(out)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error de o1 =  -0.13743\n",
      "Error de o2 =  0.03964\n"
     ]
    }
   ],
   "source": [
    "error_o1 = round((target[0] - a_out_o1) * a_out_o1 * (1 - a_out_o1), 5)\n",
    "error_o2 = round((target[1] - a_out_o2) * a_out_o2 * (1 - a_out_o2), 5)\n",
    "\n",
    "print(\"Error de o1 = \", error_o1)\n",
    "print(\"Error de o2 = \", error_o2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error total\n",
    " \n",
    "Para calcular el error de la red se utiliza la función del error cuadrático medio\n",
    "\n",
    "$Error_{total} = \\frac{1}{2} \\Sigma_{i=0}^m (target_i - a_i^{(out))})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0204583345\n"
     ]
    }
   ],
   "source": [
    "error_total = error_o1**2 + error_o2**2\n",
    "print(error_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo Backpropagation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez calculado el error específico de la capa de salida, se puede realizar el ajuste de pesos entre la capa de salida y la capa oculta $W^{(out)}$\n",
    "\n",
    "El ajuste de los pesos se realiza de la siguiente forma:\n",
    "\n",
    "$w_{i,j}^{(out)} = w_{i,j} + \\eta * error_{a_j^{(out)}} * (1 - error_{a_j^{(out)}}) * a_i^{(h)} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambio de pesos en la capa de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35353633 0.45339565 0.5218415 ]\n",
      " [0.46131553 0.56134979 0.61903434]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.5\n",
    "\n",
    "# Pesos conectados a la neurona o1\n",
    "w_out[0][0] = w_out[0][0] + eta * error_o1 * (1 - error_o1) * a_h_h1\n",
    "w_out[0][1] = w_out[0][1] + eta * error_o1 * (1 - error_o1) * a_h_h2\n",
    "w_out[0][2] = w_out[0][2] + eta * error_o1 * (1 - error_o1) * 1\n",
    "\n",
    "# Pesos conectados a la neurona o2\n",
    "w_out[1][0] = w_out[1][0] + eta * error_o2 * (1 - error_o2) * a_h_h1\n",
    "w_out[1][1] = w_out[1][1] + eta * error_o2 * (1 - error_o2) * a_h_h2\n",
    "w_out[1][2] = w_out[1][2] + eta * error_o2 * (1 - error_o2) * 1\n",
    "\n",
    "\n",
    "print(w_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de realizar el ajuste de los pesos de la capa oculta, es necesario realizar el calculo del error específico de las neuronas de la capa oculta:\n",
    "\n",
    "$error_{a_i^{(h)}} = \\Sigma_{j = 0}^n \\big( error_{a_j^{(out)}} * w_{i,j} * a_i^{(h)} * (1 - a_i^{(h)}) \\big )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error de h1:  -0.003058379333270399\n",
      "Error de h2:  -0.0054081234454247985\n"
     ]
    }
   ],
   "source": [
    "error_h1 = error_o1 * w_h[0][0] * a_h_h1 * (1 - a_h_h1) + \\\n",
    "           error_o2 * w_h[1][0] * a_h_h1 * (1 - a_h_h1)\n",
    "\n",
    "error_h2 = error_o1 * w_h[0][1] * a_h_h2 * (1 - a_h_h2) + \\\n",
    "           error_o2 * w_h[1][1] * a_h_h2 * (1 - a_h_h2)\n",
    "\n",
    "print(\"Error de h1: \", error_h1)\n",
    "print(\"Error de h2: \", error_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El cambio de pesos en la capa oculta se realiza de la misma forma que en la capa de salida, pero ahora considerando los errores específicos de la capa oculta.\n",
    "\n",
    "$w_{i,j}^{(h)} = w_{i,j} + \\eta * error_{a_j^{(h)}} * (1 - error_{a_j^{(h)}}) * a_i^{(in)} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.14992331 0.24984661 0.34846613]\n",
      " [0.19986407 0.29972813 0.34728131]]\n"
     ]
    }
   ],
   "source": [
    "w_h[0][0] = w_h[0][0] + eta * error_h1 * (1 - error_h1) * a_in_i1\n",
    "w_h[0][1] = w_h[0][1] + eta * error_h1 * (1 - error_h1) * a_in_i2\n",
    "w_h[0][2] = w_h[0][2] + eta * error_h1 * (1 - error_h1) * 1\n",
    "\n",
    "w_h[1][0] = w_h[1][0] + eta * error_h2 * (1 - error_h2) * a_in_i1\n",
    "w_h[1][1] = w_h[1][1] + eta * error_h2 * (1 - error_h2) * a_in_i2\n",
    "w_h[1][2] = w_h[1][2] + eta * error_h2 * (1 - error_h2) * 1\n",
    "\n",
    "\n",
    "print(w_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "395a4e92152996666fb2f9ccdfaa123fbee5cdfd349ce9639909b685c816356c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
